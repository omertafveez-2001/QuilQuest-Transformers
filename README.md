# Transformers-for-Language-Modelling
We will be using a **Multi-Head Self Attention** (MHSA) Transformer to build a model for Sequence 2 Sequence generation of language using a text dataset gathered from different novels including Lord of the Rings, Game of Thrones and Harry Potter.

This simple architecture is replicated from the paper **"Attention is All You Need"** by Vaswani, 2017. 

### **Important Libraries needed to import**
- nlkt
- tiktoken
- Pytorch
