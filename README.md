# Transformers-for-Language-Modelling
We will be using a **Multi-Head Self Attention** (MHSA) Transformer to build a model for Sequence 2 Sequence generation of language using a text dataset gathered from different novels including Lord of the Rings, Game of Thrones and Harry Potter.
